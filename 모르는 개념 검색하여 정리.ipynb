{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"모르는 개념 검색하여 정리.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMrAhNYXpVqwh0lIt/kW0Gg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2y3a3nqeXug0"},"source":["# lookup table(순람표, 룩업 테이블)\r\n","- 컴퓨터 과학에서 일반적으로 배열이나 연관 배열로 된 데이터 구조\r\n","- 간단한 배열 인덱싱 동작으로 검색"]},{"cell_type":"markdown","metadata":{"id":"393Qm3HTYF1R"},"source":["# Word Embedding\r\n","- $W(\\text{\"cat\"}) = (0,2, -0,4, 0.7, \\cdots) \\\\ W(\\text{\"mat\"} = (0.0, 0.6, -0.1, \\cdots)$\r\n","- 단어를 고차원의 벡터로 보내주는 함수\r\n"]},{"cell_type":"markdown","metadata":{"id":"x6Z7hCcoYutL"},"source":["# Representation Learning(표현 학습)\r\n","- Representation ? 모델의 각 층(multi-scaled)에서 나오는 feature map들의 집합을 의미.\r\n","- 데이터의 복잡한 구조를 multiple level로 바꿔주는 것\r\n","- 최적의 특징을 자동으로 알아낸다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"tMBFDrIeaiRz"},"source":["# Curse of dimensionality(차원의 저주)\r\n","- 데이터의 차원 증가 -> 해당 공간의 크기(부피) 기하급수적으로 증가 -> 동일한 갯수의 데이터의 밀도 급속도로 감소\r\n","- 따라서 차원이 증가할수록 데이터의 분포 분석, 모델 추정에 필요한 샘플 데이터의 갯수가 기하급수적으로 증가"]},{"cell_type":"markdown","metadata":{"id":"uZpQN4W4uyCA"},"source":["# Transfer Learning(전이 학습)\r\n","- 특정 환경에서 만들어진 AI 알고리즘을 다른 **비슷한 분야**에 적용하는 것\r\n","- ex) 체스 AI 에게 장기를 두게 한다. 사과깎는 AI에게 배를 깎게 한다."]},{"cell_type":"markdown","metadata":{"id":"ZkECIMYH7rLK"},"source":["# Downsampling\r\n","- 차원을 줄여서 적은 메모리로 깊은 convolution을 할 수 있게 하는 것\r\n","- 보통 stride를 2 이상으로 하는 convolution 층을 사용하거나 pooling 층을 사용 -> 이 과정에서 어쩔 수 없이 feature의 정보를 잃음\r\n","- Downsampling하는 부분을 Encoder라고 한다.\r\n","- 입력받은 이미지의 정보 -> 인코더 -> 압축된 벡터 출력"]},{"cell_type":"markdown","metadata":{"id":"OoTpcfKG8EDN"},"source":["# Upsampling\r\n","- Downsampling을 통해서 받은 결과의 차원을 늘려서 input과 같은 차원으로 만들어주는 과정\r\n","- 주로 strided transpose convolution을 사용\r\n","- Upsampling하는 부분을 Decoder라고 한다.\r\n","- 압축된 벡터 -> 디코더 -> 입력받은 이미지와 크기가 동일한 결과물 출력"]},{"cell_type":"markdown","metadata":{"id":"rbEOEqov8MTP"},"source":["# Covariate(공변량)\r\n","- 여러 변수들이 공통적으로 함께 공유하고 있는 변량. 종속변수에 대하여 독립변수와 기타 잡음인자들이 공유하는 변량.\r\n","- 연구를 진행하는 데 잡음인자가 있을 경우, 독립변수의 순수한 영향력을 검출해낼 수 없으므로 공변량을 이용하여 잡음인자를 통제.\r\n","- 공변량 분석의 목적 : 독립변수 이외의 잡음인자들이 조속변수에 영향을 미치는 것을 통제함으로써 독립변수 자체의 순수한 영향을 측정하는 것\r\n"]},{"cell_type":"markdown","metadata":{"id":"phvrMlvtDLTN"},"source":["# Internal Covariate Shift\r\n","- <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/3.png\" width=800>\r\n","- 딥러닝 네트워크에서 hidden layer를 거칠 때마다 각 층의 Activation에 영향을 받아 각 층마다의 input의 분포가 달라지는 현상\r\n","- Batch Normalization 층을 더하거나, hidden layer들에 대한 input을 정상화함으로써 문제 해결"]},{"cell_type":"markdown","metadata":{"id":"Wn9JAw0izpyB"},"source":["# Normalization(정규화)\r\n","- 신경망의 학습을 빠르게 할 수 있는 방법들 중 하나는 입력 데이터를 정규화하는 것.\r\n","- 입력 데이터를 정규화하지 않으면 손실 함수가 최저값으로 수렴하는 데 오랜 시간이 걸림. 또한, learning rate(학습률)을 크게 설정한다면 값이 발생할 위험이 있어 까다로움.\r\n","- 입력 데이터를 정규화해주게 된다면 -> 손실 함수의 수렴이 빠르게 이루어지고 안정적이게 된다.\r\n","- unnormalized는 앞뒤로 왔다갔다 하면서 수 많은 단계를 거쳐 최적값에 도달한다. 또한, learning  rate(학습률)을 작게 설정해야 한다.\r\n","- normalized는 어디서 시작하든 쉽게 최적값에 도달할 수 있다. learning rate(학습률)을 상대적으로 높혀서 사용할 수 있기 때문에 빠르게 훈련이 가능하다."]},{"cell_type":"markdown","metadata":{"id":"K9AipVUCDV4P"},"source":["# Batch Normalization(배치 정규화)\r\n","- 논문 버전\r\n","  - Internal Covariate Shift 현상을 막기 위한 방법\r\n","  - <img src=\"https://shuuki4.files.wordpress.com/2016/01/bn1.png\">\r\n","  - 학습 단계 : 각각의 배치 단위의 평균, 분산으로 정규화\r\n","    - 정해지지 X 값!\r\n","  - 추론 단계(테스트 단계) : 계산해놓은 이동 평균, 이동 분산 또는 지수 평균, 지수 분산으로 정규화\r\n","    - 정해진 값!\r\n","    - 이동 평균(moving average), 이동 분산(moving variance) -> 최근 N개의 배치의 데이터만 사용\r\n","    - 지수 평균, 지수 분산 -> 전체 데이터 사용\r\n","    - 주로, 이동 평균과 이동 분산을 사용한다.\r\n","  - 정규화한 이후에는 scale factor($\\gamma$), shift factor($\\beta$)를 이용하여 새로운 값 출력\r\n","  - $\\gamma, \\beta$ 파라미터\r\n","    - 학습 단계에서 backpropagation(역전파) 과정에서 학습되는 파라미터\r\n","    - 단순히 표준정규분포를 따르게 만드는 것이 아니라 우리가 원하는 분포를 따르도록 만들 수 있다. 또한, ReLU 활성화 함수를 거치게 되면 0보다 작은 것은 다 무시하는데 그 무시하는 부분을 줄일 수 있게 이동시킬 수도 있다.\r\n","  - 배치 정규화는 하이퍼파라미터 탐색을 쉽게 만들어줄 뿐만 아니라, 신경망과 하이퍼파라미터의 상관관계를 줄여준다.\r\n","- 유투브 본 설명 버전\r\n","  - 가중치 w,b를 빠르게 학습하기 위해서 layer의 출력값인 a를 정규화하는 것이 효과적이다. \r\n","  - 하지만, 활성화 함수를 거친 후인 a를 정규화하는 것보다 활성화 함수를 거치기 전인 z를 정규화하는 것이 더 많이 사용되고 있다. \r\n","  - 그렇다면 어떻게 배치 정규화를 구현하는가? \r\n","  - 위의 그림처럼 z의 평균, 분산을 구한다. 그 후, 정규화된 z($z_{norm}$을 구하게 되는 데 $z_{norm}$은 항상 평균이 0, 분산이 1인 분포를 따른다. \r\n","  - 하지만! 신경망의 은닉층의 분포가 전부 비슷해지게 되면 은닉층을 깊게 쌓는 의미가 없어지기 때문에 은닉층의 분포가 다양해야 한다. \r\n","  - 그래서 만들어진 방법이 배치 정규화. 그래서 $z_{norm}$ 대신해서 $\\hat{z} = \\gamma*z_{norm}+\\beta$를 사용한다.\r\n","  - 그렇게 되면 경사적 하강법, momentum, RMSprop, Adam을 이용한 경사하강법 등의 알고리즘을 사용하여 $\\gamma, \\beta$를 업데이트(학습)시킨다. -> $\\gamma, \\beta$의 값에 따라 $\\hat{z}$의 평균, 분산을 마음대로 설정 가능하게 된다.\r\n","  - 즉, $\\gamma, \\beta$의 값을 변화시키면서 은닉층의 값($z_{i}$)들이 서로 다른 평균, 분산의 분포를 따르도록 할 수 있다는 것이다! 이게 바로 배치 정규화의 핵심!\r\n","  - 즉, 요약하자면 은닉 유닛이 표준화된 평균, 분산을 갖되 평균, 분산은 학습 알고리즘에서 설정할 수 있는 두 변수 $\\gamma, \\beta$에 의해 조절된다.\r\n","  - 은닉층의 입력값의 분포를 내가 원하는 대로 바꿔줄 수 있다!\r\n","  - 참고 : https://www.youtube.com/watch?v=tNIpEZLv_eg\r\n","- 가장 간단한 설명 버전\r\n","  - <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/4.png\" width=800>\r\n","  - 이렇게 같은 층이더라도 배치 단위에 따라서 분포가 다른 것을 같은 층 내에서는 분포가 모두 같아지게! 각각의 원래 배치 단위의 평균, 분산을 이용해서 정규화하는 것을 배치 정규화!\r\n","  - 참고 : https://gaussian37.github.io/dl-concept-batchnorm/\r\n","\r\n","- 궁금증\r\n","  1. 왜 activation 층 이전에 배치 정규화 층을 두는가?\r\n","  - 이유 : 배치 정규화의 목적 자체가 네트워크의 각 층의 연산 결과가 우리가 원하는 방향의 분포대로 나오게 하는 것. variance가 크게 되어 gradient exploding, gradient vanishing 과 같은 문제가 생기지 않게 variance를 조절해주는 것.\r\n","  - 그래서 activation 층에 들어가기 전의 분포가 많이 바뀌지 않도록 activation 층 이전에 사용한다.\r\n","  - 주로, convolution layer나 fully-connected layer와 같이 핵심 연산을 한 후 분포가 많이 변화되어있으니까 그 직후에 batch normalization 층을 사용한다.\r\n","  2. 그렇다면 배치 정규화 층이 우리가 원하는 분포를 따르도록 바꿔줄 수 있으니까 아무 분포나 설정해도 되는가?\r\n","  - 이유 : 아니다. 주로 표준정규분포를 따르도록 (평균:0, 분산:1) 한다. 그 이유는 활성화 함수 ReLU, Sigmoid 등의 함수를 보면\r\n","  - <img src=\"https://miro.medium.com/max/666/1*nrxtwp6rzqdFhgYh0x-eVw.png\" width=800>\r\n","  - 주로 0 근처의 값에서 의미있다.\r\n","  - 또한, 배치 정규화 층의 목적 자체가 variance가 너무 크지 않게(크게 되면 네트워크의 층을 지날수록 점점 차이가 더 벌어지므로) 조절하는 것이므로 작은 variance를 사용하는 것이 좋다. 단, 너무 작은 variance를 사용하게 되면 분포가 0에 가깝게 수렴하게 되므로 너무 작은 variance가 아닌, 경험적으로 적절한 variance인 1을 사용하는 것이다.\r\n","\r\n","- batch normalization 의 장점\r\n"," - Internal Covariate Shift 문제를 개선하고자 하는 방법에는 Weight Initialization 방법을 사용하는 것, learning rate를 작게 조절하는 것 등이 있는 데 이러한 방법들에는 또 다른 문제가 발생하기 때문에\r\n"," - batch normalization을 사용함으로써 그러한 또 다른 문제를 발생시키지 않고 Internal Covariate Shift 문제를 개선할 수 있다.\r\n"," - 또한, overfitting 문제를 방지할 수 있는 regularization(정규화) 효과도 있다.\r\n","  "]},{"cell_type":"markdown","metadata":{"id":"Q8v3AQt7z6J6"},"source":["# 정규화 VS 배치 정규화\r\n","- 정규화 : 신경망의 입력값을 정규화\r\n","- 배치 정규화 : 신경망 안의 은닉층을 정규화\r\n","  - 은닉층의 입력값인 $z_{i}$의 분포(평균, 분산)을 정규화\r\n","  - 은닉층의 입력값. 즉 feature의 scale을 맞춰준다.\r\n","    - 그로 인한 효과? feature의 scale이 다 다르다면 -> gradient descent에 따른 weight의 영향이 각자 다르기 때문에\r\n","    - gradient의 편차가 클 때는 gradient exploding의 문제가 발생할 수 있고\r\n","    - gradient의 편차가 작을 때는 gradient vanishing의 문제가 발생할 수 있다.\r\n","  - 따라서, 정규화를 통해 gradient descent에 따른 weight의 반응을 다 맞춰줄 수 있다."]},{"cell_type":"markdown","metadata":{"id":"n1G_oQ8bt6pz"},"source":["# Gradient Descent(경사 하강법) VS Stochastic Gradient Descent(SGD, 확률적 경사 하강법)\r\n","- 전체 데이터로 한 번에 학습할 때 -> 경사 하강법 사용\r\n","  - 경사 하강법 : 데이터셋의 모든 데이터를 다 사용하여 학습.\r\n","  - 모든 gradient를 구하고 그 모든 gradient를 평균낸 다음에 업데이트\r\n","- 미니배치 단위로 학습할 때 -> 확률적 경사 하강법 사용\r\n","  - 확률적 경사 하강법 : 미니배치 단위로 배치마다 학습.\r\n","  - 그 뭉치마다의 gradient를 구하고 그 gradient들을 평균낸 다음에 업데이트"]},{"cell_type":"markdown","metadata":{"id":"5j753Oh1LkzI"},"source":[""]}]}