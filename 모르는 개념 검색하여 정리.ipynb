{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"모르는 개념 검색하여 정리.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP0LiJvhZbKb4uBn4RiNP/3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2y3a3nqeXug0"},"source":["# lookup table(순람표, 룩업 테이블)\r\n","- 컴퓨터 과학에서 일반적으로 배열이나 연관 배열로 된 데이터 구조\r\n","- 간단한 배열 인덱싱 동작으로 검색"]},{"cell_type":"markdown","metadata":{"id":"393Qm3HTYF1R"},"source":["# Word Embedding\r\n","- $W(\\text{\"cat\"}) = (0,2, -0,4, 0.7, \\cdots) \\\\ W(\\text{\"mat\"} = (0.0, 0.6, -0.1, \\cdots)$\r\n","- 단어를 고차원의 벡터로 보내주는 함수\r\n"]},{"cell_type":"markdown","metadata":{"id":"x6Z7hCcoYutL"},"source":["# Representation Learning(표현 학습)\r\n","- Representation ? 모델의 각 층(multi-scaled)에서 나오는 feature map들의 집합을 의미.\r\n","- 데이터의 복잡한 구조를 multiple level로 바꿔주는 것\r\n","- 최적의 특징을 자동으로 알아낸다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"tMBFDrIeaiRz"},"source":["# Curse of dimensionality(차원의 저주)\r\n","- 데이터의 차원 증가 -> 해당 공간의 크기(부피) 기하급수적으로 증가 -> 동일한 갯수의 데이터의 밀도 급속도로 감소\r\n","- 따라서 차원이 증가할수록 데이터의 분포 분석, 모델 추정에 필요한 샘플 데이터의 갯수가 기하급수적으로 증가"]},{"cell_type":"markdown","metadata":{"id":"uZpQN4W4uyCA"},"source":["# Transfer Learning(전이 학습)\r\n","- 특정 환경에서 만들어진 AI 알고리즘을 다른 **비슷한 분야**에 적용하는 것\r\n","- ex) 체스 AI 에게 장기를 두게 한다. 사과깎는 AI에게 배를 깎게 한다."]},{"cell_type":"markdown","metadata":{"id":"ZkECIMYH7rLK"},"source":["# Downsampling\r\n","- 차원을 줄여서 적은 메모리로 깊은 convolution을 할 수 있게 하는 것\r\n","- 보통 stride를 2 이상으로 하는 convolution 층을 사용하거나 pooling 층을 사용 -> 이 과정에서 어쩔 수 없이 feature의 정보를 잃음\r\n","- Downsampling하는 부분을 Encoder라고 한다.\r\n","- 입력받은 이미지의 정보 -> 인코더 -> 압축된 벡터 출력"]},{"cell_type":"markdown","metadata":{"id":"OoTpcfKG8EDN"},"source":["# Upsampling\r\n","- Downsampling을 통해서 받은 결과의 차원을 늘려서 input과 같은 차원으로 만들어주는 과정\r\n","- 주로 strided transpose convolution을 사용\r\n","- Upsampling하는 부분을 Decoder라고 한다.\r\n","- 압축된 벡터 -> 디코더 -> 입력받은 이미지와 크기가 동일한 결과물 출력"]},{"cell_type":"markdown","metadata":{"id":"rbEOEqov8MTP"},"source":["# Covariate(공변량)\r\n","- 여러 변수들이 공통적으로 함께 공유하고 있는 변량. 종속변수에 대하여 독립변수와 기타 잡음인자들이 공유하는 변량.\r\n","- 연구를 진행하는 데 잡음인자가 있을 경우, 독립변수의 순수한 영향력을 검출해낼 수 없으므로 공변량을 이용하여 잡음인자를 통제.\r\n","- 공변량 분석의 목적 : 독립변수 이외의 잡음인자들이 조속변수에 영향을 미치는 것을 통제함으로써 독립변수 자체의 순수한 영향을 측정하는 것\r\n"]},{"cell_type":"markdown","metadata":{"id":"phvrMlvtDLTN"},"source":["# Internal Covariate Shift\r\n","- 딥러닝 네트워크에서 hidden layer를 거칠 때마다 각 층의 Activation에 영향을 받아 각 층마다의 input의 분포가 달라지는 현상\r\n","- Batch Normalization 층을 더하거나, hidden layer들에 대한 input을 정상화함으로써 문제 해결"]},{"cell_type":"markdown","metadata":{"id":"K9AipVUCDV4P"},"source":["# Batch Normalization(배치 정규화)\r\n","- Internal Covariate Shift 현상을 막기 위한 방법\r\n","- <img src=\"https://shuuki4.files.wordpress.com/2016/01/bn1.png\">\r\n","- 학습할 때 : 미니배치의 평균, 분산으로 정규화\r\n","- 테스트할 때 : 계산해놓은 이동 평균으로 정규화\r\n","- 정규화한 이후에는 scale factor, shift factor를 이용하여 새로운 값을 만들고 출력\r\n","- 배치 정규화는 하이퍼파라미터 탐색을 쉽게 만들어줄 뿐만 아니라, 신경망과 하이퍼파라미터의 상관관계를 줄여준다."]},{"cell_type":"markdown","metadata":{"id":"Wn9JAw0izpyB"},"source":["# Normalization(정규화)\r\n","- 신경망의 학습을 빠르게 할 수 있는 방법들 중 하나는 입력 데이터를 정규화하는 것.\r\n","- 입력 데이터를 정규화하지 않으면 손실 함수가 최저값으로 수렴하는 데 오랜 시간이 걸림. 또한, learning rate(학습률)을 크게 설정한다면 값이 발생할 위험이 있어 까다로움.\r\n","- 입력 데이터를 정규화해주게 된다면 -> 손실 함수의 수렴이 빠르게 이루어지고 안정적이게 된다.\r\n","- unnormalized는 앞뒤로 왔다갔다 하면서 수 많은 단계를 거쳐 최적값에 도달한다. 또한, learning  rate(학습률)을 작게 설정해야 한다.\r\n","- normalized는 어디서 시작하든 쉽게 최적값에 도달할 수 있다. learning rate(학습률)을 상대적으로 높혀서 사용할 수 있기 때문에 빠르게 훈련이 가능하다."]},{"cell_type":"code","metadata":{"id":"Q8v3AQt7z6J6"},"source":[""],"execution_count":null,"outputs":[]}]}