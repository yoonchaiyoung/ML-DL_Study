{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"모르는 개념 검색하여 정리.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYiuoQZwCEXGXhYfh8Pnax"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2y3a3nqeXug0"},"source":["# lookup table(순람표, 룩업 테이블)\r\n","- 컴퓨터 과학에서 일반적으로 배열이나 연관 배열로 된 데이터 구조\r\n","- 간단한 배열 인덱싱 동작으로 검색"]},{"cell_type":"markdown","metadata":{"id":"393Qm3HTYF1R"},"source":["# Word Embedding\r\n","- $W(\\text{\"cat\"}) = (0,2, -0,4, 0.7, \\cdots) \\\\ W(\\text{\"mat\"} = (0.0, 0.6, -0.1, \\cdots)$\r\n","- 단어를 고차원의 벡터로 보내주는 함수\r\n"]},{"cell_type":"markdown","metadata":{"id":"x6Z7hCcoYutL"},"source":["# Representation Learning(표현 학습)\r\n","- Representation ? 모델의 각 층(multi-scaled)에서 나오는 feature map들의 집합을 의미.\r\n","- 데이터의 복잡한 구조를 multiple level로 바꿔주는 것\r\n","- 최적의 특징을 자동으로 알아낸다.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"tMBFDrIeaiRz"},"source":["# Curse of dimensionality(차원의 저주)\r\n","- 데이터의 차원 증가 -> 해당 공간의 크기(부피) 기하급수적으로 증가 -> 동일한 갯수의 데이터의 밀도 급속도로 감소\r\n","- 따라서 차원이 증가할수록 데이터의 분포 분석, 모델 추정에 필요한 샘플 데이터의 갯수가 기하급수적으로 증가"]},{"cell_type":"markdown","metadata":{"id":"uZpQN4W4uyCA"},"source":["# Transfer Learning(전이 학습)\r\n","- 특정 환경에서 만들어진 AI 알고리즘을 다른 **비슷한 분야**에 적용하는 것\r\n","- ex) 체스 AI 에게 장기를 두게 한다. 사과깎는 AI에게 배를 깎게 한다."]},{"cell_type":"markdown","metadata":{"id":"ZkECIMYH7rLK"},"source":["# Downsampling\r\n","- 차원을 줄여서 적은 메모리로 깊은 convolution을 할 수 있게 하는 것\r\n","- 보통 stride를 2 이상으로 하는 convolution 층을 사용하거나 pooling 층을 사용 -> 이 과정에서 어쩔 수 없이 feature의 정보를 잃음\r\n","- Downsampling하는 부분을 Encoder라고 한다.\r\n","- 입력받은 이미지의 정보 -> 인코더 -> 압축된 벡터 출력"]},{"cell_type":"markdown","metadata":{"id":"OoTpcfKG8EDN"},"source":["# Upsampling\r\n","- Downsampling을 통해서 받은 결과의 차원을 늘려서 input과 같은 차원으로 만들어주는 과정\r\n","- 주로 strided transpose convolution을 사용\r\n","- Upsampling하는 부분을 Decoder라고 한다.\r\n","- 압축된 벡터 -> 디코더 -> 입력받은 이미지와 크기가 동일한 결과물 출력"]},{"cell_type":"markdown","metadata":{"id":"rbEOEqov8MTP"},"source":["# Covariate(공변량)\r\n","- 여러 변수들이 공통적으로 함께 공유하고 있는 변량. 종속변수에 대하여 독립변수와 기타 잡음인자들이 공유하는 변량.\r\n","- 연구를 진행하는 데 잡음인자가 있을 경우, 독립변수의 순수한 영향력을 검출해낼 수 없으므로 공변량을 이용하여 잡음인자를 통제.\r\n","- 공변량 분석의 목적 : 독립변수 이외의 잡음인자들이 조속변수에 영향을 미치는 것을 통제함으로써 독립변수 자체의 순수한 영향을 측정하는 것\r\n"]},{"cell_type":"markdown","metadata":{"id":"phvrMlvtDLTN"},"source":["# Internal Covariate Shift\r\n","- <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/3.png\" width=800>\r\n","- 딥러닝 네트워크에서 hidden layer를 거칠 때마다 각 층의 Activation에 영향을 받아 각 층마다의 input의 분포가 달라지는 현상\r\n","- Batch Normalization 층을 더하거나, hidden layer들에 대한 input을 정상화함으로써 문제 해결"]},{"cell_type":"markdown","metadata":{"id":"Wn9JAw0izpyB"},"source":["# Normalization(정규화)\r\n","- 신경망의 학습을 빠르게 할 수 있는 방법들 중 하나는 입력 데이터를 정규화하는 것.\r\n","- 입력 데이터를 정규화하지 않으면 손실 함수가 최저값으로 수렴하는 데 오랜 시간이 걸림. 또한, learning rate(학습률)을 크게 설정한다면 값이 발생할 위험이 있어 까다로움.\r\n","- 입력 데이터를 정규화해주게 된다면 -> 손실 함수의 수렴이 빠르게 이루어지고 안정적이게 된다.\r\n","- unnormalized는 앞뒤로 왔다갔다 하면서 수 많은 단계를 거쳐 최적값에 도달한다. 또한, learning  rate(학습률)을 작게 설정해야 한다.\r\n","- normalized는 어디서 시작하든 쉽게 최적값에 도달할 수 있다. learning rate(학습률)을 상대적으로 높혀서 사용할 수 있기 때문에 빠르게 훈련이 가능하다."]},{"cell_type":"markdown","metadata":{"id":"K9AipVUCDV4P"},"source":["# Batch Normalization(배치 정규화)\r\n","- 논문 버전\r\n","  - Internal Covariate Shift 현상을 막기 위한 방법\r\n","  - <img src=\"https://shuuki4.files.wordpress.com/2016/01/bn1.png\">\r\n","  - 학습할 때 : 미니배치의 평균, 분산으로 정규화\r\n","  - 테스트할 때 : 계산해놓은 이동 평균으로 정규화\r\n","  - 정규화한 이후에는 scale factor, shift factor를 이용하여 새로운 값을 만들고 출력\r\n","  - 배치 정규화는 하이퍼파라미터 탐색을 쉽게 만들어줄 뿐만 아니라, 신경망과 하이퍼파라미터의 상관관계를 줄여준다.\r\n","- 유투브 본 설명 버전\r\n","  - 가중치 w,b를 빠르게 학습하기 위해서 layer의 출력값인 a를 정규화하는 것이 효과적이다. \r\n","  - 하지만, 활성화 함수를 거친 후인 a를 정규화하는 것보다 활성화 함수를 거치기 전인 z를 정규화하는 것이 더 많이 사용되고 있다. \r\n","  - 그렇다면 어덯게 배치 정규화를 구현하는가? \r\n","  - 위의 그림처럼 z의 평균, 분산을 구한다. 그 후, 정규화된 z($z_{norm}$을 구하게 되는 데 $z_{norm}$은 항상 평균이 0, 분산이 1인 분포를 따른다. \r\n","  - 하지만! 신경망의 은닉층의 분포가 전부 비슷해지게 되면 은닉층을 깊게 쌓는 의미가 없어지기 때문에 은닉층의 분포가 다양해야 한다. \r\n","  - 그래서 만들어진 방법이 배치 정규화. 그래서 $z_{norm}$ 대신해서 $\\hat{z} = \\gamma*z_{norm}+\\beta$를 사용한다.\r\n","  - 그렇게 되면 경사적 하강법, momentum, RMSprop, Adam을 이용한 경사하강법 등의 알고리즘을 사용하여 $\\gamma, \\beta$를 업데이트(학습)시킨다. -> $\\gamma, \\beta$의 값에 따라 $\\hat{z}$의 평균, 분산을 마음대로 설정 가능하게 된다.\r\n","  - 즉, #\\gamma, \\beta#의 값을 변화시키면서 은닉층의 값($z_{i}$)들이 서로 다른 평균, 분산의 분포를 따르도록 할 수 있다는 것이다! 이게 바로 배치 정규화의 핵심!\r\n","  - 즉, 요약하자면 은닉 유닛이 표준화된 평균, 분산을 갖되 평균, 분산은 학습 알고리즘에서 설정할 수 있는 두 변수 $\\gamma, \\beta$에 의해 조절된다.\r\n","  - 은닉층의 입력값의 분포를 내가 원하는 대로 바꿔줄 수 있다!\r\n","  - 참고 : https://www.youtube.com/watch?v=tNIpEZLv_eg\r\n","- 가장 간단한 설명 버전\r\n","  - <img src=\"https://gaussian37.github.io/assets/img/dl/concept/batchnorm/4.png\" width=800>\r\n","  - 이렇게 같은 층이더라도 배치 단위에 따라서 분포가 다른 것을 같은 층 내에서는 분포가 모두 같아지게! 각각의 원래 배치 단위의 평균, 분산을 이용해서 정규화하는 것을 배치 정규화!\r\n","  - 참고 : https://gaussian37.github.io/dl-concept-batchnorm/"]},{"cell_type":"markdown","metadata":{"id":"Q8v3AQt7z6J6"},"source":["# 정규화 VS 배치 정규화\r\n","- 정규화 : 신경망의 입력값을 정규화\r\n","- 배치 정규화 : 신경망 안의 은닉층을 정규화\r\n","  - 은닉층의 입력값인 $z_{i}$의 분포(평균, 분산)을 정규화"]},{"cell_type":"code","metadata":{"id":"n1G_oQ8bt6pz"},"source":[""],"execution_count":null,"outputs":[]}]}