{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoEncoder.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPxpPWz4CNd2OWGvLTlAqTM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZBStsMiVAmCX"},"source":["참고 서적 : 파이썬 날코딩으로 알고 짜는 딥러닝 - 윤덕호 지음 (출판사 : 한빛미디어)"]},{"cell_type":"markdown","metadata":{"id":"KbqGUmvUA9rv"},"source":["# AutoEncoder?\n","- 입력을 재현하는 방식의 비지도학습\n","- Encoder, Decoder 두 부분으로 구성\n","- 잘 학습된 AutoEncoder의 Encoder는 적은 양의 labeling된 데이터만을 이용하는 지도학습, 미리 정해진 category없이 데이터 내용만으로 유사한 데이터를 찾는 semantic hashing 등 다양한 용도로 이용 가능"]},{"cell_type":"markdown","metadata":{"id":"HPSXN9PZBsxK"},"source":["목차\n","- 오토인코더의 구조, 원리\n","- 오토인코더의 활용 : 준지도학습, 잡음 제거, 시맨틱 검색\n","- 지도학습 기능과 시맨틱 해싱 기능이 추가된 확장 오토인코더 모델\n","- 필기체 숫자 이미지 분류를 위한 엠니스트 데이터셋"]},{"cell_type":"markdown","metadata":{"id":"kIC1rSrzCHNP"},"source":["# 오노인코더의 구조\n","<img src=\"https://blog.kakaocdn.net/dn/ZAS36/btqzZKBCb3X/yvuxoSPOMQRd2nHufcR0S1/img.png\" width=600>\n","\n","- AutoEncoder는 Encoder, Decoder 두 부분으로 구성\n","- Encoder의 입력 형태 = Decoder의 출력 형태\n","- AutoEncoder의 목적 : 입력과 최대한 비슷한 출력을 만들어내는 방향으로 학습\n","- 출력은 입력과 동일한 형태 & 내용도 최대한 비슷하게 재현(representation)해야 한다.\n","- 손실함수(loss function) : MSE(평균 제곱 오차) 이용"]},{"cell_type":"markdown","metadata":{"id":"4HX6zBFTCJo0"},"source":["## Encoder\n","- 입력 처리\n","- 입력의 내부 중간 표현(code)으로 만듦\n","- <img src=\"https://gaussian37.github.io/assets/img/dl/concept/autoencoder1/autoencoder.png\" width=600>"]},{"cell_type":"markdown","metadata":{"id":"cn3P8Mv7DIKV"},"source":["## Decoder\n","- Encoder가 생성한 code를 처리하여 출력을 만듦"]},{"cell_type":"markdown","metadata":{"id":"UbYHTzM0DQyS"},"source":["오토인코더의 목적이 입력과 비슷한 출력을 만들어내는 것만이 목적의 전부? -> 입력을 출력으로 복사해버리면 끝.\n","\n","하지만, 오토인코더의 실제 목적 : 입력을 code로 변환해주는 **Encoder**를 쓸모있는 모습으로 학습시키는 것!\n","\n","code 의 형태\n","- 신경망 설계자가 마음대로 정할 수 있음.\n","- 입력에 비해 작은 크기로 정하는 것이 보통 -> 이렇게 해야지 오토인코더가 의미를 가짐.\n","- code의 크기가 입력의 크기와 같거나 크다면 -> 모든 입력 성분을 코드에 복사해둔 후 다시 출력에 복사하는 방식으로 쉽게 입력을 재현할 수 있음.\n","- 입력의 크기 > code의 크기 -> Encoder가 입력 정보를 압축!\n","\n","그 압축과정에서 입력 데이터들이 갖는 유용하고 의미 있는 패턴들을 code로 포착. -> 오토인코더를 이용해 중간 표현을 만들어가면서 학습시키는 이유"]},{"cell_type":"markdown","metadata":{"id":"05CGA4-7DeiJ"},"source":["# 지도학습 VS 비지도학습\n","AutoEncoder\n","- 입력과 최대한 비슷한 출력을 만들어내려는 과정에서\n","- 입력 데이터 사이에 숨어있는 유용한 **패턴**을 찾아내기 바라는 구조\n","\n","머신러닝 분야에서의 학습 - 지도학습, 비지도학습으로 나뉨.\n","\n","지도학습\n","- 교사에게 배우는 학습\n","- label 정보 이용O\n","비지도학습\n","- 독학 공부\n","- label 정보 이용X\n","\n","학습용 데이터 모으는 일, 수집된 데이터에 일일이 labeling하는 일 -> 인력, 비용 많이 든다.\n","\n","게다가 같은 데이터이더라도 신경망 용도가 달라지면 그에 따라 부착해야할 label 내용이 달라짐 -> 새로운 labeling 작업 필요 -> 지도학습은 비용 많이 든다.\n","\n","AutoEncoder를 이용한 학습 -> labeling 작업 필요 X, 비지도 학습\n","\n","입력 자체가 label 정보의 역할을 한다.\n","\n","AutoEncoder의 장점\n","- 데이터만 수집 -> labeling 작업 필요X -> 바로 작업 가능\n","\n","하지만, 그냥 AutoEncoder는 필요X\n","\n","그냥 입력값을 그대로 복사하는 것이 더 편리\n","\n","하지만, 잘 학습된 AutoEncoder에서 Encoder를 분리하여 활용 방안을 찾으면 이야기가 달라짐.\n","\n","대표적인 방법 : 약간의 labeling된 데이터를 이용해 지도학습 수행.\n","\n","Encoder가 만들어내는 내부 code 속에 정말로 입력의 유용한 패턴들이 압축 표현되어있다? -> 적은 양의 labeling 된 데이터만으로도 높은 품질의 학습 결과를 얻을 수 있다.\n","\n","이처럼, AutoEncoder의 Encoder는 지도학습 신경망의 하부 구조로 이용가능\n","\n","오토인코더 속에서 학습되어 얻어진 인코더의 파라미터 내용르 고정된 값으로 삼아 지도학습 신경망의 파라미터만 학습 가능, 가변적인 파라미터 초깃값으로 간주하여 지도학습 과정에서 추가로 변경되게 할 수 O\n","\n","비지도학습, 지도학습의 결합 -> 다양한 용도의 지도학습이 필요할 때 더욱 효과적\n","\n","Encoder를 하나만 학습시켜 여러 분야의 지도학습에 다용도로 이용할 수 있기 때문\n","\n","비용 절감 효과!"]},{"cell_type":"markdown","metadata":{"id":"g3s74OXiKkO-"},"source":["# AutoEncoder가 활용될 수 있는 분야"]},{"cell_type":"markdown","metadata":{"id":"FrjyyDX3GGWj"},"source":["## 잡음 제거용 AutoEncoder\n","기본 구조의 오토인코더, 약간의 변형이 추가된 오토인코더 활용 가능\n","\n","그 중 잡음 제거용 오토인코더\n","\n","입력 -> **잡음 주입기** -> 변형된 입력 -> **인코더** -> 코드 -> **디코더** -> 출력(입력 재현)\n","\n","주의할 점!\n","- decoder 출력에 대한 손실 함수(loss function)은 encoder에 주어지는 변형된 입력이 아니라 잡음 주입 이전의 **원본 입력**과 비교하여 계산해야 한다!\n","- 이렇게 학습을 시켜야지만 encoder + decoder인 AutoEncoder 부분이 변형된 입력에서 잡음을 제거해 원본 데이터로 회복시키는 기능을 갖게 된다.\n","\n","## 잡음 주입\n","- 두 계층 사이에 잡음 주입 계층을 삽입하여 이 계층이 아래 계층의 출력에 적당한 형태의 잡음을 추가하여 위 계층에 전달하는 정규화 기법\n","- 잡음이 주입되면 위 계층의 학습은 그만큼 혼란을 겪지만 -> 더욱 강건한 학습이 이루어져 다양한 입력의 변이를 더 잘 처리 O\n","- 입력의 변이에 대한 강건한 학습을 가능하게 하며, 평가 단계에서의 품질을 높여준다.\n","- 매번 다른 잡음이 가미되어 다양하게 변형된 입력이 제공 -> 일정한 처리 방해 -> 모델의 과적합을 방지하는 효과 존재\n","- 잡음 주입의 형태, 빈도, 강도는 신경망 설계자가 선택\n","- 잡음 주입 -> 역전파 처리에서 따로 신경 쓸 필요 X -> 학습 대상X 때문\n","- 원래의 입력과 무관한 잡음이 더해진 입력값이 들어간 것일뿐.\n","- 순전파 처리에서의 잡음 추가는 학습 중에만 작동하게 하고, 평가 단계에서는 작동하지 않도록 해야한다. 실전에서도 굳이 잡음으로 판단을 방해할 필요 X 때문. -> 잡음 주입 계층은 학습 중인지 여부에 따라 잡음 주입 여부를 제어할 수 있어야 한다."]},{"cell_type":"markdown","metadata":{"id":"p1_ULN00Kp9F"},"source":["## 유사 이진 코드 생성과 시맨틱 해싱\n","- 시맨틱 해싱을 이용한 정보 검색 분야\n","\n","### 시맨틱 해싱(semantic hashing)\n","- 검색 대상 콘텐츠를 적당한 수의 비트 정보로 표현된 이진 벡터 공간에 대응 -> 쉽고 빠르게 정보 검색을 하는 방법\n","- ex) 척추/무척추(0xx, 1xx), 육상/해상(x0x, x1x), 육식/초식(xx0, xx1)\n","- -> 사자(000), 문어(110), 코끼리(001) 의 코드를 각자 부여받는다.\n","- 기존의 여러 콘텐츠가 이러한 기준으로 분류되어 있다 -> 호랑이에 관한 콘텐츠가 검색키로 주어졌을 때 코드값 111을 얻게 되고 -> 코드값 111에 배치된 콘텐츠를 찾을 수 있다.\n","- 해이런 검색 결과 과정이 해싱 테이블을 통해 이루어지기 때문에 마치 배열 원소를 꺼내듯 빠르게 처리될 수 있다.\n","- 코드값의 각 비트가 표현하고 있는 특징들이 일치하는 콘텐츠들이 검색 결과가 된다.\n","- 코드를 3 비트 -> 32비트로 늘리면 $2^{32}$만큼 코드에 의해 분할되는 영역이 훨씬 많이 늘어난다는 것을 알 수 있다.\n","- 영역 수가 콘텐츠양에 비해 지나치게 많다? -> 빈 영역이 많이 생기기 때문에 데이터 구조가 효율적으로 관리되도록 주의 필요!\n","- 정확히 매치되는 콘텐츠가 없을 때 유사 영역을 찾는 알고리즘도 필요\n","\n","시맨틱 해싱의 기준\n","- 적절한 이진 분류의 기준\n","- 어떻게 각각의 데이터에 이진 코드값 부여\n","\n","이러한 시맨틱 해싱의 기준 2가지를 한꺼번에 해결 -> Encoder가 생산하는 내부 코드를 시맨틱 해싱을 위한 이진 코드값으로 이용하는 것!\n","\n","그러려면 Encoder가 생성하는 내부 code가 유사 이진 코드(반드시 0과 1. 은 아니지만 0과 1 주변에 몰려 분포하는 형태가 되도록) 유도해야 한다.\n","\n","-> 해결방법? Encoder 최상단 계층의 비선형 활성화 함수로 sigmoid 함수 이용\n","\n","### Sigmoid 함수\n","- 0 ~ 1 사이의 출력을 내지만 학습이 잘 진행될수록 입력에 따라 0 또는 1에 수렴하는 값으로 출력의 분포가 몰리는 경향이 있음\n","\n","Encoder가 내부 code를 유사 이진 코드로 생성해서 학습 -> 내부 코드값을 반올림하여 -> 0 or 1로 바꿔 완전한 이진 코드를 얻을 수 있다. -> 시맨틱 해싱의 아이디어는 이렇게 구한 이진 코드를 검색 해시 키로 이용하자는 것이다.\n","AutoEncoder의 내부 코드의 각 성분은 나름대로 무언가 중요, 의미 있는 패턴을 표현하고 있을 가능성이 크다.\n","따라서 같은 이진 코드 or 비슷한 이진 코드를 갖는 항목끼리는 중요한 공통점을 갖고 있다고 기대할 수 있다.\n","\n","AutoEncoder를 이용한 시맨틱 해싱은 검색되어 나오는 항목의 유사성을 AutoEncoder 학습이 이루어지기 전에 미리 짐작할 수 없다는 점에서 더욱 흥미롭다. "]}]}