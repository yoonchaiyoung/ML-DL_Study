{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"순환신경망(RNN).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLFv8MSVZxcJxl9vXs4Mbu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ixJUzITUHTFp"},"source":["참고 : https://wikidocs.net/22886\n","\n","# 순환 신경망(Tecurrent Neural Network)\n","\n","RNN은 Sequence 모델이다. 반복적이고 순차적인 데이터 학습에 특화된 인공신경망의 한 종류로써 내부의 순환구조가 들어있다는 특징을 가지고 있다.\n","\n","입력, 출력을 시퀀스 단위로 처리.\n","\n","과거의 학습, 현재의 학습의 연결을 가능하게 한다.\n","\n","시간에 종속된다는 특징이 있다.\n","\n","ex) 번역기, 텍스트의 앞 뒤 성분 파악에 사용\n","\n","입력 : 번역하고자 하는 문장(단어 시퀀스)\n","\n","출력 : 번역된 문장(단어 시퀀스)\n","\n","이러한 시퀀스들을 처리하기 위해 고안된 모델ㅇ르 시퀀스 모델이라고 한다.\n","\n","RNN 은 딥러닝 가장 기본적인 시퀀스 모델\n","\n","LSTM, GRU도 RNN에 속한다.\n","\n","다층 퍼셉트론 신경망은 입력이 출력방향으로만 활성화된다. 은닉뉴런이 과거의 정보를 기억하지 못한다.\n","\n","## 순환신경망과 재귀신경망의 차이점\n","\n","순환신경망 : 사슬(체인)형태의 계산 그래프 사용\n","\n","재귀신경망 : 트리형태의 계산 그래프 사용"]},{"cell_type":"markdown","metadata":{"id":"8j1b3wBXI7lQ"},"source":["# 순환 신경망(RNN) 그림\n","\n","앞에서 배운 신경망들은 전부 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했다.\n","\n","이러한 신경망을 Feed Forward Neural Network라고 한다.\n","\n","그런데 이 RNN은 그렇지 않는다.\n","\n","은닉층의 노드에서 활성화함수를 통해 나온 결과값을 출력층 방향으로도 보내면서 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 가지고 있다.\n","\n","x : 입력층의 입력 벡터\n","\n","y : 출력층의 출력 벡터\n","\n","b : 편향\n","\n","셀(cell) : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드. 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행.\n","\n","= 메모리 셀. RNN 셀. 이라고도 함.\n","\n","은닉층의 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층이 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 하고 있다.\n","\n","# 순환신경망 그림 설명\n","현재 시점을 변수 t라고 표현하자.\n","\n","<img src=\"https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG\">\n","\n","현재 시점 t에서의 메모리 셀이 가지고 있는 값은\n","\n","과거의 메모리 셀들의 값에 영향을 받은 것이다.\n","\n","그러면 이 메모리 셀이 갖고 있는 이 값은 뭐라고 할까? 은닉 상태값.\n","\n","메모리 셀이 출력층 방향/ 다음 시점 t+1의 자신에게 보내는 값을 은닉 상태(hidden state)라고 한다.\n","\n","은닉 상태 : 메모리 셀이 출력층 방향으로 보내는 값, 다음시점의 자기자신에게 보내는 값\n","\n","즉, 원래 t시점으로 들어오는 입력값 뿐만 아니라\n","\n","t-1 시점의 메모리시점에서 보낸값이\n","\n","t시점의 메모리셀의 입력값으로 같이 사용된다.\n","\n","즉, t시점의 메모리셀의 입력값 2개?\n","1. 원래 t시점으로 들어오는 입력값 $x_t$\n","2. t-1 시점의 출력값.\n","\n","RNN을 표현할 때 일반적으로 화살표로 사이클을 그려서 재귀 형태로 표현하기도 하지만\n","\n","사이클을 그리는 화살표 대신에\n","\n","여러 시점으로 펼쳐서 표현하기도 한다.\n","\n","단지 사이클을 그리는 화살표를 사용했냐, 시점의 흐름에 따라서 표현했냐. 의 차이일뿐 둘 다 동일한 RNN을 표현하고 있다.\n","\n","Feed Forward Neural Network에서는 뉴런이라는 단위를 사용했다.\n","\n","RNN에서는 뉴런이라는 단위보다는\n","\n","입력층, 출력층 - 입력 벡터, 출력 벡터\n","\n","은닉층 - 은닉 상태\n","\n","라는 표현을 주로 사용한다.\n","\n","그림의 각 네모들은 기본적으로 벡터 단위를 가정하고 있다.\n","\n","<img src=\"https://wikidocs.net/images/page/22886/rnn_image2.5.PNG\">\n","\n","이렇게 시점 1에서의 은닉층의 출력값이\n","1. 출력값\n","2. 다음 시점(시점2)로의 입력값\n","\n","으로 간다는 것을 확인 할 수 있다.\n","\n","# 순환신경망 입,출력의 길이에 따른 종류\n","\n","RNN은 입력, 출력의 길이를 다르게 설계 가능\n","\n","ex) one-to-many, many-to-one, many-to-many\n","\n","RNN 셀의 각 시점별 입력, 출력의 단위 : 보통 '단위 벡터'\n","\n","사용처\n","1. one-to-many\n","- 입력 1개\n","- 출력 여러개\n","- 이미지 캡셔닝 작업 ; 하나의 이미지 입력에 대해서 사진의 제목을 출력\n","\n","2. many-to-one\n","- 입력 문서가 긍정적인지 부정적인지를 판별하는 감성분류(sentiment classification)\n","- 메일이 정상/스팸메일 판별하는 스팸 메일 분류(spam detection)\n","\n","3. many-to-many\n","- 챗봇 ; 입력 문장으로부터 대답 문장 출력\n","- 번역기 ; 입력문장으로부터 번역된 문장 출력\n","- 개체명 인식 ; 단어를 보고는 그 단어가 어떤 유형인지를 인식\n","- 품사 태깅 ; 형태소의 뜻, 문맥을 고려하여 대응되는 품사 인식"]},{"cell_type":"markdown","metadata":{"id":"6swOdMLNMc6N"},"source":["# RNN 수식 정리\n","\n","<img src=\"https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG\">\n","\n","$h_t$ : 현재 시점 t에서의 은닉 상태값\n","\n","은닉층의 메모리셀은 $h_t$ 를 계산하기 위해 2개의 가중치를 갖는다.\n","\n","1. $W_x$ : 입력층의 입력값을 위한 가중치\n","2. $W_h$  : 이전 시점 t-1의 은닉 상태값$h_{t-1}$을 위한 가중치\n","\n","# 은닉층의 활성화함수가 tanh인 이유\n","Q) 이 때, RNN은 왜 활성화함수로 tanh 함수를 사용할까?\n","\n","A) RNN의 vanishing gradient problem(기울기값이 사라지는 문제) 때문이다. \n","- 기울기값이 사라지는 문제? 역전파방법 중 기울기항이 사라지는 문제. 항이 0이나 0에 가까워져서 학습이 불가능해지는 현상.\n","- sigmoid 함수의 최대값은 0.25인데 tanh 함수의 최대값은 1로, 기울기값이 사라지는 문제 조금 완화됨\n","\n","그래서 RNN의 기울기값이 사라지는 문제를 예방하기 위해서 기울기가 최대한 오래 유지될 수 있도록 하는 역할로 tanh 함수가 적합하기 때문이다.\n","\n","sigmoid 함수의 경우 곱셈이 거듭되면서 기울기값을 잃어버리는 문제가 생길 가능성이 상대적으로 크기 때문에 tanh를 사용하는 것이 기울기값을 잃지 않는데 유리하다. 라고 말할 수 있다.\n","\n","다시, RNN 수식으로 다시 돌아가면\n","\n","# RNN 수식\n","은닉층 : $h_{t} = tanh(W_{x} x_{t} + W_{h}h_{t−1} + b)$\n","\n","출력층 : $y_{t} = f(W_{y}h_{t} + b)$\n","\n","$f$ : 비선형 활성화 함수 중 하나\n","\n","RNN 은닉층 연산을 벡터, 행렬 연산으로 이해할 수 있다.\n","\n","자연어처리의 RNN 입력 $x_t$ : 대부분 단어 벡터\n","\n","$d$ : 단어 벡터의 차원\n","\n","$D_h$ : 은닉상태값의 크기\n","\n","위의 그림에서 가중치 $W_x, W_h, W_y$의 값은 모든 시점(t)에서 값이 동일하다.\n","\n","만약, 은닉층이 2개 이상일 경우에는 은닉층 2개의 가중치는 서로 다르다.\n","\n","# 출력층에서 사용하는 활성화 함수\n","출력층에서 사용하는 활성화 함수는 상황에 따라 다른 $f$ 사용.\n","\n","이진 분류 - sigmoid 함수 사용 ; 출력값이 0.5이상이면 클래스1, 0.5이하면 클래스2로 분류하니까\n","\n","다항 분류 - softmax 함수 사용 ; 각각의 클래스에 대한 확률로 계산되니까\n","\n","# RNN 구현\n","hidden_size : 은닉 상태의 크기.\n","메모리셀(은닉층)이 다음 시점의 메모리셀과 출력층으로 보내는 값의 크기(output_dim)과 동일.\n","\n","time_steps : 시점의 수. 입력 시퀀스의 길이(input_length).\n","\n","input_dim : 입력의 크기\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8JUKYikIVq0v"},"source":["# 깊은 순환 신경망(Deep Recurrent Neural Network)\n","RNN 이 다수의 은닉층을 가지고 있을 때\n","\n","<img src=\"https://wikidocs.net/images/page/22886/rnn_image4.5_finalPNG.PNG\">\n","\n","이 그림은 은닉층이 2개인 깊은 순환 신경망의 모습"]},{"cell_type":"markdown","metadata":{"id":"N-k1Joj9V5GG"},"source":["# 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n","시점 t에서의 출력값을 예측할 때\n","\n","이전 시점의 데이터뿐만 아니라\n","\n","이후 데이터로도 예측할 수 있다는 아이디어에 기반\n","\n","RNN 이 과거 시점(과거 time step)의 데이터들만 참고해서 찾고자하는 정답을 예측하짐나\n","\n","실제 문제에서는 향후 시점의 데이터에 힌트가 잇는 경우가 많다.\n","\n","따라서 이전 시점의 데이터뿐만 아니라 이후 시점의 데이터도 힌트로 할용하기 위해 고안된. 양방향 RNN.\n","\n","<img src=\"https://wikidocs.net/images/page/22886/rnn_image5_ver2.PNG\">\n","\n","양방향 RNN은 하나의 출력값을 예측하기 위해 2개의 메모리셀을 사용한다.\n","\n","첫 번째 메모리셀 - 앞 시점(t-1)의 은닉 상태 전달받아 현재의 은닉상태 계산 (주황색 메모리셀)\n","\n","두 번째 메모리셀 - 뒤 시점(t+1)의 은닉 상태 전달받아 현재의 은닉상태 계산 (초록색 메모리셀)\n","\n","그러고나서 주황색, 초록색 메모리셀의 값 2개를 이용해서 출력층에서 출력값을 예측하기 위해 사용된다.\n","\n","<img src=\"https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG\">\n","\n","물론 이 그림처럼 양방향 RNN도 다수의 은닉층을 가질 수 있다.\n","\n","그림은 은닉층이 2개인 깊은 양방향 순환 신경망이다.\n","\n","다른 인공신경망 모델들과 마찬가지로, 은닉층을 무조건 추가한다고해서 모델의 성능이 좋아지는 것은 아니다.\n","\n","은닉층을 추가하면, 학습할 수 있는 양이 많아지지만,\n","\n","반대로 훈련데이터 또한 그만큼 많이 필요하다."]},{"cell_type":"markdown","metadata":{"id":"9XfmdUgtYpbM"},"source":["# 장단기메모리(LSTM) 모델\n","여태까지 배운 RNN 모델을 바닐라 RNN(Vanilla RNN)이라고 한다.\n","\n","RNN 중 가장 단순한 형태이기 때문이다.\n","\n","# 바닐라 RNN의 한계\n","바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다.\n","\n","하지만 바닐라 RNN은 비교적 짧은 시퀀스에 대해서만 효과가 있고, 시점(time step)이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생한다.\n","\n","맨 처음의 입력값의 정보량이 뒤로 갈수록(시점이 길어질수록) 맨 처음 입력값에 대한 정보의 영향력이 거의 없어질 수도 있다. 점차 영향력이 감소한다.\n","\n","그런데, 어쩌면 가장 중요한 정보가 시점의 맨 앞에 위치할 수도 있다. 그런데 RNN이 충분한 기억력을 가지고 있지 못한다면 다음 단어를 예측할 때 엉뚱한 답을 내놓을 확률이 높아진다.\n","\n","이를 \"장기 의존성 문제\"라고 한다."]},{"cell_type":"markdown","metadata":{"id":"-o2axrSJZW6w"},"source":["# 바닐라 RNN 내부 열어보기\n","LSTM에 대해 공부하기 전에 바닐라 RNN의 내부를 봐보자.\n","\n","<img src=\"https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG\">\n","\n","이 그림은 바닐라 RNN의 내부 구조이다.\n","\n","편향 b는 그림에서 생략했다.\n","\n","$h_{t} = tanh(W_{x}x_{t} + W_{h}h_{t−1} + b)$\n","\n","바닐라 RNN은 $x_t$와 $h_{t-1}$ 의 2개의 입력이 각각 가중치와 곱해져서 메모리셀에 입력된다.\n","\n","이를 tanh 함수의 입력값을 사용하고\n","\n","이 값은 은닉층의 출력인 은닉상태가 된다.\n","\n","시점 t-1을 보면 $h_{t-1}$가 t-1시점의 출력과 다음 시점인 t시점의 입력으로 가는 것을 볼 수 있다.\n","\n","원래는 <img src=\"https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG\"> 그림처럼 메모리셀의 출력이 다시 자기자신으로 돌아가는 재귀 화살표로 그리지만\n","\n","시점의 순서대로 펼쳐서 보면 위의 그림처럼 되므로\n","\n","이전 시점의 출력값이 현재 시점의 입력값이 된다는 것을 알 수 있다."]},{"cell_type":"markdown","metadata":{"id":"KTRxQWQDaNPQ"},"source":["# LSTM(Long Short-Term Memory)\n","<img src=\"https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG\">\n","이 그림은 LSTM 의 내부 모습이다.\n","\n","전통적인 RNN의 단점(장기의존성의 문제)를 보완한 RNN 종류이다.\n","\n","LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는 데 탁월한 성능을 보인다.\n","\n","LSTM은 은닉층의 메모리셀에 입력 게이트, 망각 게이트(삭제 게이트), 출력 게이트를 추가하여\n","\n","불필요한 기억은 지우고, 기억해야할것들을 정리한다.\n","\n","전통적인 RNN보다 좀 더 복잡해졌고 셀상태(cell state)라는 값을 추가했다.\n","\n","## 내가 헷갈린 용어\n","아까 은닉층의 결과값 : 은닉 상태\n","\n","지금 나온 단어는 셀 상태..\n","\n","# LSTM 설명\n","$C_t$ : 시점 t의 셀 상태\n","\n","<img src=\"https://wikidocs.net/images/page/22888/cellstate.PNG\">\n","\n","왼쪽에서 오른쪽으로 가는 굵은 선\n","\n","셀 상태 또한 위의 바닐라 RNN에서 배운 은닉 상태처럼 이전 시점의 셀 상태($C_{t-1}$)가 다음 시점의 셀 상태($C_{t}$)의 입력으로서 사용된다.\n","\n","즉, 셀상태도 좀 전에 배운 은닉 상태처럼\n","\n","이전 시점의 값이 다음 시점의 입력으로 사용된다.\n","\n","은닉 상태값($h_t$), 셀 상태값($C_t$)을 구하기 위해 새로 3개의 게이트를 추가한다.\n","1. 삭제 게이트\n","2. 입력 게이트\n","3. 출력 게이트\n","\n","이 3개의 게이트에는 공통적으로 sigmoid 함수가 존재한다.\n","\n","sigmoid 함수를 지나면 0~1사이의 값이 나오게 되는 데\n","\n","이 값들을 가지고 게이트를 조절한다.\n","\n","- σ : sigmoid 함수\n","- tanh : 하이퍼볼릭탄젠트 함수\n","- $W_{xi}, W_{xg}, W_{xf}, W_{xo}$ : 각 게이트에서 $x_t$와 함께 사용되는 가중치\n","- $W_{hi}, W_{hg}, W_{hf}, W_{ho}$ : 각 게이트에서 $h_{t-1}$과 함께 사용되는 가중치\n","- $b_{i}, b_{g}, b_{f}, b_{o}$ : 각 게이트에서 사용되는 편향\n","\n","참고 : https://wikidocs.net/22888"]},{"cell_type":"code","metadata":{"id":"bBrivjE8cRVl"},"source":[""],"execution_count":null,"outputs":[]}]}