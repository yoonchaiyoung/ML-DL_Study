{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet 논문 공부.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO86lhQTbPRID8lZF0ARY+P"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-rshwEmm6CMN"},"source":["# Deep Residual Learning for Image Recognition"]},{"cell_type":"markdown","metadata":{"id":"JysQm52MYf-I"},"source":["# Abstract(논문 개요)\r\n","- 심층 신경망은 훈련하기 어렵다.\r\n","- 우리는 이전에 사용된 것보다 훨씬 더 깊은 네트워크의 훈련을 쉽게하기 위해서 residual learning 프레임워크를 제시한다.\r\n","- 참조되지 않은 함수를 학습하는 것 대신에, layer의 입력값을 참조하여 residual 함수를 학습하는 것으로써 layer를 명확하게 새로 만든다.\r\n","- 우리는 종합적인 경험적 증거를 제시한다. 이 residual network는 최적화하는 데 쉽고 꽤 증가된 깊이를 통해 정확도를 얻어내는 것이 가능하다는.\r\n","- ImageNet 데이터셋에서, 우리는 VGG 네트워크보다 8배 더 깊은 최대 152개의 층으로 residual 네트워크를 평가하지만 여전히 복잡성은 낮다.\r\n","- 이러한 residual 네트워크의 앙상블은 ImageNet 테스트 데이터셋에 대하여 3.57%의 오류를 달성했다.\r\n","- 이 결과는 ILSVRC 2015 분류 과제에서 1위를 차지했다.\r\n","- 또한 CIFAR-10 데이터에 대한 분석을 100개와 1000개의 layer로 제공한다.\r\n","- 표현(representation)의 깊이는 많은 시각적 인식 과제에서 매우 중요하다.\r\n","- 단지 극도로 깊은 표현 덕분에, 우리는 COCO 객체 탐지 데이터셋에서 상대적으로 28% 향상하게 되었다.\r\n","- Deep residual 네트워크는 ILSVRC & COCO 2015 대회에 제출한 기초이다. 또한 우리는 ImageNet 탐지, ImageNet localization, COCO 탐지, COCO segmentation의 과제에서 1등을 한 이유이다."]},{"cell_type":"markdown","metadata":{"id":"2ebXfd2TY9Vd"},"source":["# 1. Introduction\r\n","- deep convolutional neural network는 이미지 분류(image classification)의 일련의 돌파구라고 여겨졌다.\r\n","- deep network는 자연스럽게 저/중/고 수준의 feature과 종단간 다층 방식(end-to-end multilayer fashion)으로 통합시켰다."]},{"cell_type":"code","metadata":{"id":"0w4LyK_kdLFm"},"source":[""],"execution_count":null,"outputs":[]}]}